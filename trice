1. pretraining
在预处理方面，最长用的手段是tf-idf， 这里pretraining既是指数tf-idf的过程，这里的trick即是，将所有train test 一起训练， 即共用一个词表，这样不会出现为登录词。 
这个方法是kaggle做文本方面最最常用的方法，基本做过比赛的人都知道， 不算犯规， 但是很有效果， 一本结果大约能够在不这么训练的基础上提高2%左右，非常有效的一种方法。

2. bigram
在一般的文本中， 每个词作为一个feature有的时候显得对于约束那么强。举个例子，比如推荐 系统， 这个就算两个词好了， 他们单独分类有单独的表义性， 但是如果不单独分开的化，很明显的是一个专有名词——推荐系统，在这个时候我们不将他分开效果会很好， 大概会有1%左右的提升。 主要是其联合的表义性。这在kaggle上也是一种很常见的技巧。

3. logistic regression
这是文本分类里，可能用处最多的模型，其实并不是他的训练效果多么好， 主要是训练速度快，模型适用的场合多， 所以被各个选手所常用。 LR是可以用工具包的，当然自己写也不慢， 因为LR是可以用稀疏矩阵实现的。 
在这里我主要想说的是LR一定要加正则项， 对于文本分类来说l2 是必须的， l1可以用来选feature。 
对比与svm ， svm的优点是确实训练结果比lr好， 但是对evaluation 的比赛来说，svm的适用性很弱， svm的概率解是用交叉验证调出来的，速度非常慢， 非常不推荐使用。 而且对于解会有概率解释， 比较方便和其他模型进行融合。

4. 模型融合
在这里我想说的不是技巧，而是教训，主要是我对于模型融合一点都不会。 我手调了一组8分类器线性融合的模型， 结果非常惨不忍睹， 所以这个方法没有实战经验的同学们 还是别浪费时间了， 好好练好一个分类器才是正经道理。

5. random forest 和knn
knn可以做benchmark， benchmark的问题我后面会讲， rf 的优点是有bagging 这种相对来说很优秀的东西， 但是做文本分类特别容易过拟合，非常不推荐。

6. benchmark
knn和rf都是比较好的benchmark， 在用你所认为比较好的方法做kaggle之前，非常推荐用一个效果不是很好，但是很稳定，不是很依赖feature的方法做一下， 其实基本前几次交你就能确定分类器和feature有什么特点了。

7. feature
跟中秋大神交流的时候他老人家告诉我， 说kaggle上最重要的是feature， 其次才是别的, 听了之后深有感触， 确实feature选的好，结果确实不一样，说穿了分类器基本能用上的都不到5个， 正常做回归的化基本上就只用ridge， 这种情况下feature的选取至关重要。 

8. lda
lda是在大家都没有办法的时候采用， lda的优点就是一般维度都比较小300 以下，而且效果拔群。 推荐用mallet，这个包确实比较好用。 lda的缺点是—— 不稳定， 我指的不稳定是在当结果要搞到小数点后三位的时候（一般也就是20名左右）不稳定， 如果你tf-idf 才 80多， 请大胆的适用lda。

9. lda tf-idf
顾名思义， 这两个feature可以混着用， 效果也会非常不错，但是lda必须老试， 要不很多时候都不好使。

10. 交叉验证
这种东西是kaggle上最恶心的东西， 提交次数的限制导致必须交叉验证， 交叉验证带来的问题是——结果非常不稳定， 根本看不出来最优解一般在哪儿，经验上说，一般别信交叉验证的， 如果一般结果特别不好的时候是真的， 其他都不可信。
